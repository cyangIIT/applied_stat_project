---
title: "Chao and Albert Applied Project"
author: "Albert Mandizha, Chao Yang"
date: "2022-11-30"
output: html_document
---

# Multiple Linear Regression
# 1. Clean Environment & Load Libraries
```{r}
library(caret)
library(caTools)
library(dplyr)

```

# 2. Source, Collect and Load Data

## 2.1 Source and Collect Data

Our data is collected online based on existed numbers.

## 2.2 Load Data

Importing the dataset
```{r}
dataset = read.csv("50_Startups.csv")
plot(dataset)
```

# 3. Data preprocessing and Data cleaning

## 3.1 Explore missing value patterns
## 3.2 Check if there are NA values

```{r}
NANumbers <- sum(is.na(dataset)) 
paste("**** The number of NA values in this data set =", NANumbers)
# Plotting percentage of missing values per feature
library(naniar)
gg_miss_var(dataset, show_pct = TRUE)
```

We don't have missing values but we drop na values just to check

# 3.3 Data parsing (Set dummy variables)

We don't have dummy variables to set in our dataset

## 3.3.1 Encoding categorical data

```{r}
dataset$State = factor(dataset$State,
                       levels = c('New York', 'California', 'Florida'),
                       labels = c(1, 2, 3))
dataset$State=as.integer(dataset$State)
# Display new data frame
head(dataset)
```

## 3.4 Data histograms
```{r}
par(mfrow=c(1,1))
hist(dataset$R.D.Spend, col = "wheat",main = "Research and Development Spending",xlab = "R&D Spending")
hist(dataset$Administration, col = "skyblue",main = "Administration",xlab = "Administration Cost")
hist(dataset$Marketing.Spend, col = "green", main = "Marketing Spend",xlab = "Marketing Spending")
hist(dataset$State, col = "coral2",main = "State",xlab = "State")
```
## 3.4.1 Kernel Density Plot

Density curves allow us to quickly see whether or not a graph is left skewed, right skewed, or has no skew. We see there are no features that their density have no skew.

```{r}
library(ggplot2)
par(mfrow=c(2,3))
plot(density(dataset$R.D.Spend))
plot(density(dataset$Administration))
plot(density(dataset$Marketing.Spend))
plot(density(dataset$State))
plot(density(dataset$Profit))
```

## 3.5 Checking & Treating Outliers

```{r}
boxplot(dataset$R.D.Spend,
        ylab = "R.D.Spend")
boxplot(dataset$Administration,
        ylab = "Administration Cost")
boxplot.stats(dataset$Administration)$out
boxplot(dataset$Marketing.Spend,
        ylab = "Marketing Spending")
boxplot(dataset$State,
        ylab = "State")
```

It can be noted that from the boxplots there are no outliers from the dataset.

## 3.6 Data Correlation Analysis

```{r}
library(corrplot)
corrplot(cor(dataset), addCoef.col = 'black', method="color")
```


#4.0 Data Analysis Stage

## 4.1 Splitting the dataset into the Training set and Test set

```{r}
library(caret)
set.seed(123)
training.samples=dataset$Profit%>%
  createDataPartition(p=.8, list =FALSE)
training_set=dataset[training.samples,]
test_set=dataset[-training.samples,]
```


## 4.2 Model Fitting Multiple Linear Regression to the Training set

```{r}
library(Metrics)
regressor = lm(formula = Profit ~ .,
               data = training_set)
summary(regressor)
predy = predict(regressor, newdata = test_set)
mse <- mse(training_set$Profit,predy)
cat("MSE" , mse, "\n")
coef(regressor)
confint(regressor)
```

## 4.3 Hypothesis test on coefficients of alpha value is setting as 0.05

```{r}
#H0: Bi = 0, i =0,1,2,3,4 
#Ha: Bi != 0
#Reject Ha fs F0 > F(0.05,4,37)
F_critical = qf(p=.05, df1=4, df=37, lower.tail=FALSE)
F_statistic = 177.1
F_critical < F_statistic
#Hence we reject H0, which means there are relationships between independent variables and dependent variables.
```

## 4.4 Preparing X and Y vectors for lasso regression

```{r}
x_train <- model.matrix(training_set$Profit~., data = training_set)[, -1]
y_train <- training_set$Profit
```

## 4.5 Displaying how the coefficients vary with lambda

```{r}
library(glmnet)
lasso_model <- glmnet(x_train, y_train, alpha = 1)
plot(lasso_model, "lambda")
plot(lasso_model, "norm")
set.seed(4)
grid <- 10^seq(2, -2, length = 100)
lasso_cv_model <- cv.glmnet(x_train, y_train, alpha = 1, lambda = grid)
plot(lasso_cv_model)

best_lambda <- lasso_cv_model$lambda.min
cat("Best Lambda: ", best_lambda, "\n")
coef(lasso_cv_model)
```

## 4.6 Performance on test data

```{r}
library(Metrics)
x_test <- model.matrix(test_set$Profit~. , data = test_set)[, -1]
lasso_pred <- predict(lasso_cv_model, s = best_lambda, newx = x_test)
test_mse <- mse(test_set$Profit, lasso_pred)
cat("Test MSE" , test_mse, "\n")
```

The resulting model have coefficients for all predictor variables.All the coefficients for other predictors got smaller with the except Intercept. This was expected as lasso performs feature selection through shrinking irrelevant coefficients to 0

# 5.0 Model Accuracy

## 5.1. Make predictions

```{r}
predictions <- regressor %>% predict(test_set)
```

## 5.2 Model performance

### 5.2.1 (a) Compute the prediction error, RMSE

```{r}
RMSE(predictions, test_set$Profit)
```

### 5.2.2 (b) Compute R-square

```{r}
R2(predictions, test_set$Profit)
```

## 5.3  Conclusion

From the output above, the R2 is 0.98, meaning that the observed and the predicted outcome values are highly correlated, which is very good.

```{r}
Error_rate= 9534.568/mean(test_set$Profit)
Error_rate
```

The prediction error RMSE is 9534.568, representing an error rate of 9534.568/mean(test_set$Profit)  = 8.3%, which is good.
